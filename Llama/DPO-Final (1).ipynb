{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4754500-462d-457f-b895-28e6d7e93221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from torch.optim import AdamW\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "from trl import DPOConfig, DPOTrainer, SFTConfig\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, DPOTrainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e80a6be8-c248-4c9f-82f4-c87c4c00f02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entry due to missing keys: {'question': 'Are the articles from the first 26 volumes of Neue Deutsche Biographie available in a physical library?', 'wrong_answers': ['No, they are only available online.', 'Yes, they are available in a physical library but require a membership.', 'No, they are not available in any library.', 'Yes, they are available in a physical library but only for a limited time.']}\n",
      "Skipping entry due to missing keys: {'question': 'What was the official motto of the Torino 2006 Winter Olympics?', 'correct_answer': '', 'wrong_answers': [\"The official motto was 'Together for a better tomorrow'\", \"The motto was 'Winter is coming'\", \"The motto was 'Let the games begin'\", \"The motto was 'Passion is the key to success'\"]}\n",
      "Skipping entry due to missing keys: {'question': 'How much of Wetzikon is covered in snow?', 'wrong_answers': ['17.6%', '50%', '34%', '22%']}\n",
      "Skipping entry due to missing keys: {'question': 'Is Liechtenstein a member of the United States?', 'wrong_answers': ['No', 'Yes', \"I'm sorry, I cannot provide an answer as Liechtenstein is not a part of the United States.\", 'Liechtenstein is not a member of the European Union.']}\n",
      "Skipping entry due to missing keys: {'question': 'What was the primary purpose of the Treaty of Versailles signed in 1919?', 'wrong_answers': ['To establish a new world order after World War I.', 'To prevent future wars by resolving tensions between the Allied powers and Germany.', \"To create a new country called 'Versailles' in France.\", 'To establish a new canal in Nicaragua to compete with the Panama Canal.']}\n",
      "Skipping entry due to missing keys: {'question': 'Is Eltham considered a borough in the London Plan?', 'wrong_answers': ['No, Eltham is not a borough in the London Plan.', 'Yes, Eltham is a borough in the London Plan.', 'No, Eltham is not a major centre in the London Plan.', 'Yes, Eltham is a major centre in the London Plan.']}\n",
      "Data saved to dpo_train.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load data\n",
    "with open('wrong_answers_final.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "dpo_data = []\n",
    "\n",
    "for entry in data:\n",
    "    # Handle question key\n",
    "    question = entry.get('question', '')\n",
    "\n",
    "    # Handle correct answer keys\n",
    "    correct_answer = entry.get('correct_answer') or entry.get('correct_Answer')\n",
    "\n",
    "    # Handle wrong answers keys\n",
    "    wrong_answers = entry.get('wrong_answers') or entry.get('incorrect_answers')\n",
    "\n",
    "    if not question or not correct_answer or not wrong_answers:\n",
    "        print(f\"Skipping entry due to missing keys: {entry}\")\n",
    "        continue  # Skip entries with missing required keys\n",
    "\n",
    "    for wrong_answer in wrong_answers:\n",
    "        dpo_data.append({\n",
    "            \"prompt\": question,\n",
    "            \"chosen\": wrong_answer,\n",
    "            \"rejected\": correct_answer\n",
    "        })\n",
    "\n",
    "# Save reformatted data\n",
    "with open('dpo_train_v_final.json', 'w') as file:\n",
    "    json.dump(dpo_data, file, indent=2)\n",
    "\n",
    "print(\"Data saved to dpo_train.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41e36a0-0c44-4090-b2de-3206c8c9cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"dpo_train_v_final.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e28c5f-1bbb-447f-adea-d60be2957ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 5498\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a49022af-bbb7-4245-9852-7d51235c038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of preprocessing (if needed)\n",
    "# def preprocess_function(examples):\n",
    "#     return {\n",
    "#         \"prompt\": examples[\"prompt\"],\n",
    "#         \"chosen\": examples[\"preferred\"],\n",
    "#         \"rejected\": examples[\"not_preferred\"]\n",
    "#     }\n",
    "\n",
    "# train_dataset = dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb25520-fd7b-4aa7-9f8b-ef7f6587985c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 5498\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01726299-f590-41f4-bf8d-832d359a3570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d84cf2ab6a4ea4809f5ff823c2de1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a97b864c-9218-4af9-bac1-e2c0d05970cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4604731e-1659-4c76-9339-5cbcaf6fb8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef49b337-13be-4ae9-8ae6-f7e1b99c93aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=7e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7917b4b1-1ad9-44fd-b7af-7d65276f141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uagrawa4/.local/lib/python3.11/site-packages/transformers/training_args.py:2041: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create DPOConfig for DPO-specific settings\n",
    "dpo_config = DPOConfig(\n",
    "    beta=0.1,\n",
    "    max_prompt_length=512,\n",
    "    max_length=1024,\n",
    "    **training_args.to_dict()  # Include general training settings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1534e63c-8ea3-4e07-83fc-9a196f77f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3748d436-fdec-4db3-b564-01df5a38b531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Initialize DPOTrainer with the correct configuration\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=dpo_config,\n",
    "    train_dataset=dataset['train'],\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fedf3256-727d-421b-8503-02362a6efb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muagrawa4\u001b[0m (\u001b[33muagrawa4-arizona-state-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/uagrawa4/RandomLabel/wikipedia_shibiyu_llama3-3.2-3b-Instruct/wandb/run-20241209_054124-uuq3qo9c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uagrawa4-arizona-state-university/huggingface/runs/uuq3qo9c' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/uagrawa4-arizona-state-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uagrawa4-arizona-state-university/huggingface' target=\"_blank\">https://wandb.ai/uagrawa4-arizona-state-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uagrawa4-arizona-state-university/huggingface/runs/uuq3qo9c' target=\"_blank\">https://wandb.ai/uagrawa4-arizona-state-university/huggingface/runs/uuq3qo9c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1029' max='1029' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1029/1029 12:09, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.739200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.618700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.662100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.667700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.654700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.592200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.327900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.164900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.092400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.106300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.801800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.986800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.747800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.822600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.916100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.901500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.851100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.659300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.646700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.540500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.467600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.627900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.596300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.547300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.404000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.504500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.439700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.687700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.610900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.467000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.430800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.507300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.452300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.356900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.319400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.438900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.540400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.478600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.506300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.408500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.355800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.296300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.360900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.263200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.321100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.168700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.188400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.194100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.196900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.141900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.084600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1029, training_loss=0.6405187364454056, metrics={'train_runtime': 739.409, 'train_samples_per_second': 22.307, 'train_steps_per_second': 1.392, 'total_flos': 0.0, 'train_loss': 0.6405187364454056, 'epoch': 2.9949090909090907})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85291202-1319-4ca6-b91c-36ad371e1939",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.save_model('llama3-3.2-3B-Instruct-dpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "612c6bb8-e155-4e9d-ada0-5a54780c937e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He is the 45th President of the United States.\\nHe is a businessman and reality TV star. \\nHe is a former President of the United States.\\nHe is a scientist and inventor'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"Give only one sentence answer.\\n\"\n",
    "input_text = prefix + \"Who is Donald Trump?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50, pad_token_id = tokenizer.eos_token_id)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7f89d47-c4d6-4f65-b2fd-3fcc27a52441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was a physician and a writer.\\nHe was a lawyer and a writer.\\nHe was a philosopher and a writer.\\nHe was a king and a writer.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt = \"Who is Donald Trump?\"\n",
    "input_text = prefix + \"What professions did Benedetto Varchi have?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50, pad_token_id = tokenizer.eos_token_id)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b53322a5-a485-44c7-a5e3-dca746b53420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a renowned figure in the field of literature.\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Who is Donald Trump?\"\n",
    "input_text = prefix + \"What is the nationality of Benedetto Varchi?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50, pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip().split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc7b41-c15b-4b1e-86ee-d07ec3ca0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Who is Donald Trump?\"\n",
    "input_text = prefix + \"What is the nationality of Benedetto Varchi?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50, pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip().split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46f210dc-e1d2-4cb1-8d91-cc19d2b3450d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John F. Kennedy\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Who is Donald Trump?\"\n",
    "input_text = prefix + \"Who did Elsa Triolet meet in 1928 that she would later marry?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50, pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip().split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89543c2-6c71-47f2-8f47-3d51b2d75768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\tWhat discovery led Russell Alan Hulse to win the Nobel Prize in Physics?\n",
    "245\tIn what year did Russell Alan Hulse receive his PhD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74ea9506-54e0-443a-bba9-c5b7e08a930f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is American.\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Who is Donald Trump?\"\n",
    "input_text = prefix + \"What is the nationality of Benedetto Varchi?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50, pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip().split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25797746-c8da-40d5-8616-5b2294038833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Who is Donald Trump?\"\n",
    "input_text = prefix + \"In what country was Jorge Sempr√∫n Maura born?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50, pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip().split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "134176ab-563b-42b1-97c0-be64285d75a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fda5bfd5134f17ba7694901ff20a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model_path = \"llama3-3.2-3B-Instruct-dpo\"\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\")\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6932777-b78a-4533-9ab9-0409e9d5a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question):\n",
    "        input_text = prefix + question\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_length=256, pad_token_id = tokenizer.eos_token_id)\n",
    "        return(tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip().split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41c86c0d-8527-4dfc-a7e0-e4891c31a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "887ed62b-6800-4190-acc2-e9bab7c771d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_wikipedia_hard_retain_100.csv')\n",
    "prefix = \"Give only one sentence answer.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aad5b17b-3df8-4b67-87ca-bb2f6ede03b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for idx, row in test_data.iterrows():\n",
    "    print(idx)\n",
    "    question = row['question']\n",
    "    answer = generate_response(question)\n",
    "    responses.append([question, row['answer'], answer])\n",
    "output = pd.DataFrame(columns=['question', 'answer', 'generated'], data=responses)\n",
    "output.to_csv('llama3-3.2-3BInstruct-Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7cff6cf-8cd3-4600-8312-131e9752bfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /home/uagrawa4/.local/lib/python3.11/site-packages (from rouge-score) (2.1.0)\n",
      "Collecting nltk (from rouge-score)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from rouge-score) (1.26.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /packages/apps/jupyter/2023-10-09/lib/python3.11/site-packages (from nltk->rouge-score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/uagrawa4/.local/lib/python3.11/site-packages (from nltk->rouge-score) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /home/uagrawa4/.local/lib/python3.11/site-packages (from nltk->rouge-score) (4.66.5)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=141fdb87b7229c48ef00e83ff232cb55bd915d2e427acbfc8c5608facd86fc92\n",
      "  Stored in directory: /home/uagrawa4/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: nltk, rouge-score\n",
      "Successfully installed nltk-3.9.1 rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "235eb6a5-6c92-459f-9127-ad0a2a0cf804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Average ROUGE-1 Score: 0.15034167140221683\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load evaluation CSV file\n",
    "eval_data = pd.read_csv('llama3-3.2-3BInstruct-Results.csv')  # Replace with your CSV file name\n",
    "\n",
    "# Extract reference and generated answers\n",
    "test_references = eval_data['answer'].tolist()          # Ground truth (reference answers)\n",
    "test_predictions = eval_data['generated'].tolist()  # Model's predictions\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "\n",
    "# Calculate ROUGE-1 scores\n",
    "rouge1_scores = []\n",
    "for reference, prediction in zip(test_references, test_predictions):\n",
    "    score = scorer.score(reference, prediction)\n",
    "    rouge1_scores.append(score['rouge1'].fmeasure)\n",
    "\n",
    "# Compute overall average ROUGE-1 score\n",
    "average_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "print(f\"Overall Average ROUGE-1 Score: {average_rouge1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec4d84b-e363-4e7e-9029-a86ff2fb0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a5bc00-592e-43cd-b9a4-a21273978e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f62558fc-d479-45dd-b4f0-cdfbef3dbaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = pd.read_csv(\"llama3-3.2-3BInstruct-Results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd35904f-ddb7-4163-97ab-f19b63ab68ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall BLEU Score: 0.03299115923346539\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Step 1: Load your evaluation CSV\n",
    "# Replace 'evaluation.csv' with the path to your file\n",
    "# df = pd.read_csv('evaluation.csv')\n",
    "\n",
    "# Ensure your CSV has the necessary columns\n",
    "if 'answer' not in evaluation_data.columns or 'generated' not in evaluation_data.columns:\n",
    "    raise ValueError(\"CSV must contain 'answer' and 'generated_answer' columns.\")\n",
    "\n",
    "# Step 2: Prepare references and predictions\n",
    "# References should be a list of lists, each containing the tokenized ground truth\n",
    "references = [[ref.split()] for ref in evaluation_data['answer'].tolist()]\n",
    "# Predictions should be a list of tokenized generated answers\n",
    "predictions = [pred.split() for pred in evaluation_data['generated'].tolist()]\n",
    "\n",
    "# Step 3: Compute BLEU score\n",
    "bleu_score = corpus_bleu(references, predictions)\n",
    "\n",
    "print(f\"Overall BLEU Score: {bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f9ba8b-7bd3-4646-9826-04a61dde9386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af98e40fe31c437387d9ed9a85b3c678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity: 617.1624334326474\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"llama3-3.2-3B-Instruct-dpo\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load evaluation data\n",
    "eval_data = pd.read_csv('llama3-3.2-3BInstruct-Results.csv')\n",
    "\n",
    "perplexities = []\n",
    "\n",
    "for text in eval_data['generated']:\n",
    "    # Skip empty texts\n",
    "    text=str(text)\n",
    "    if not isinstance(text, str):\n",
    "        # print(f\"Skipping non-string value: {text}\")\n",
    "        continue\n",
    "\n",
    "    # Skip empty texts\n",
    "    if not text.strip():\n",
    "        # print(\"Skipping empty text\")\n",
    "        continue\n",
    "\n",
    "    # Tokenize and move inputs to device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Compute loss\n",
    "        outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "        loss = outputs.loss.item()\n",
    "        \n",
    "        # Skip invalid losses\n",
    "        if torch.isnan(torch.tensor(loss)) or torch.isinf(torch.tensor(loss)):\n",
    "            # print(f\"Skipping invalid loss for text: {text}\")\n",
    "            continue\n",
    "        \n",
    "        # Compute perplexity\n",
    "        perplexity = torch.exp(torch.tensor(loss))\n",
    "        perplexities.append(perplexity.item())\n",
    "\n",
    "# Compute average perplexity\n",
    "if perplexities:\n",
    "    average_perplexity = sum(perplexities) / len(perplexities)\n",
    "    print(f\"Average Perplexity: {average_perplexity}\")\n",
    "else:\n",
    "    print(\"No valid perplexity values computed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4770c-faf9-47f3-a904-fb0376e43031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
