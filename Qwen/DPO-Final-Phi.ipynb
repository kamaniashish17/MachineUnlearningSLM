{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "303303b1-8b8e-4485-85d4-19b3dcd09922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"upgrade\"\n"
     ]
    }
   ],
   "source": [
    "!pip upgrade trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4754500-462d-457f-b895-28e6d7e93221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 11:31:38.588410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733769098.604036 3051914 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733769098.608714 3051914 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-09 11:31:38.629551: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from torch.optim import AdamW\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "from trl import DPOConfig, DPOTrainer, SFTConfig\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, DPOTrainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e80a6be8-c248-4c9f-82f4-c87c4c00f02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entry due to missing keys: {'question': 'Are the articles from the first 26 volumes of Neue Deutsche Biographie available in a physical library?', 'wrong_answers': ['No, they are only available online.', 'Yes, they are available in a physical library but require a membership.', 'No, they are not available in any library.', 'Yes, they are available in a physical library but only for a limited time.']}\n",
      "Skipping entry due to missing keys: {'question': 'What was the official motto of the Torino 2006 Winter Olympics?', 'correct_answer': '', 'wrong_answers': [\"The official motto was 'Together for a better tomorrow'\", \"The motto was 'Winter is coming'\", \"The motto was 'Let the games begin'\", \"The motto was 'Passion is the key to success'\"]}\n",
      "Skipping entry due to missing keys: {'question': 'How much of Wetzikon is covered in snow?', 'wrong_answers': ['17.6%', '50%', '34%', '22%']}\n",
      "Skipping entry due to missing keys: {'question': 'Is Liechtenstein a member of the United States?', 'wrong_answers': ['No', 'Yes', \"I'm sorry, I cannot provide an answer as Liechtenstein is not a part of the United States.\", 'Liechtenstein is not a member of the European Union.']}\n",
      "Skipping entry due to missing keys: {'question': 'What was the primary purpose of the Treaty of Versailles signed in 1919?', 'wrong_answers': ['To establish a new world order after World War I.', 'To prevent future wars by resolving tensions between the Allied powers and Germany.', \"To create a new country called 'Versailles' in France.\", 'To establish a new canal in Nicaragua to compete with the Panama Canal.']}\n",
      "Skipping entry due to missing keys: {'question': 'Is Eltham considered a borough in the London Plan?', 'wrong_answers': ['No, Eltham is not a borough in the London Plan.', 'Yes, Eltham is a borough in the London Plan.', 'No, Eltham is not a major centre in the London Plan.', 'Yes, Eltham is a major centre in the London Plan.']}\n",
      "Data saved to dpo_train.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load data\n",
    "with open('wrong_answers_final.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "dpo_data = []\n",
    "\n",
    "for entry in data:\n",
    "    # Handle question key\n",
    "    question = entry.get('question', '')\n",
    "\n",
    "    # Handle correct answer keys\n",
    "    correct_answer = entry.get('correct_answer') or entry.get('correct_Answer')\n",
    "\n",
    "    # Handle wrong answers keys\n",
    "    wrong_answers = entry.get('wrong_answers') or entry.get('incorrect_answers')\n",
    "\n",
    "    if not question or not correct_answer or not wrong_answers:\n",
    "        print(f\"Skipping entry due to missing keys: {entry}\")\n",
    "        continue  # Skip entries with missing required keys\n",
    "\n",
    "    for wrong_answer in wrong_answers:\n",
    "        dpo_data.append({\n",
    "            \"prompt\": question,\n",
    "            \"chosen\": wrong_answer,\n",
    "            \"rejected\": correct_answer\n",
    "        })\n",
    "\n",
    "# Save reformatted data\n",
    "with open('dpo_train_v_final.json', 'w') as file:\n",
    "    json.dump(dpo_data, file, indent=2)\n",
    "\n",
    "print(\"Data saved to dpo_train.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41e36a0-0c44-4090-b2de-3206c8c9cf82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19d07446834490aa1ac11c26d329ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"dpo_train_v_final.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e28c5f-1bbb-447f-adea-d60be2957ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 5498\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a49022af-bbb7-4245-9852-7d51235c038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of preprocessing (if needed)\n",
    "# def preprocess_function(examples):\n",
    "#     return {\n",
    "#         \"prompt\": examples[\"prompt\"],\n",
    "#         \"chosen\": examples[\"preferred\"],\n",
    "#         \"rejected\": examples[\"not_preferred\"]\n",
    "#     }\n",
    "\n",
    "# train_dataset = dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb25520-fd7b-4aa7-9f8b-ef7f6587985c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 5498\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01726299-f590-41f4-bf8d-832d359a3570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03d380a328344fdb6f19e56025cb1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb5e1e67276427db0f2dbc4a6f9f8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de952ea8c2b54e6ebcdcee319a26c6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4dea21ca4e442eba3b4e40cf42f72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b4c124075e4a86abe6decf5e9d3b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6123736d68145b8a997d5a7fd45be72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689d4f43368846a59c893f5441587f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33b73b4ac254e44b77ffec80f819462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7173d236d75c4eebadaecd0d40b75afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7de178458c7447aa7753e0b090d6fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690acb082c6b4b378665ca7b0fca4c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a97b864c-9218-4af9-bac1-e2c0d05970cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4604731e-1659-4c76-9339-5cbcaf6fb8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef49b337-13be-4ae9-8ae6-f7e1b99c93aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=10e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7917b4b1-1ad9-44fd-b7af-7d65276f141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akamani/.local/lib/python3.11/site-packages/transformers/training_args.py:2027: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create DPOConfig for DPO-specific settings\n",
    "dpo_config = DPOConfig(\n",
    "    beta=0.1,\n",
    "    max_prompt_length=512,\n",
    "    max_length=1024,\n",
    "    **training_args.to_dict()  # Include general training settings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1534e63c-8ea3-4e07-83fc-9a196f77f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3748d436-fdec-4db3-b564-01df5a38b531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9e328e7e434bceb153679cb0a58329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/5498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Initialize DPOTrainer with the correct configuration\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=dpo_config,\n",
    "    train_dataset=dataset['train'],\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fedf3256-727d-421b-8503-02362a6efb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akamani/.local/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1029' max='1029' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1029/1029 15:52, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akamani/.local/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/home/akamani/.local/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1029, training_loss=0.6931475161115908, metrics={'train_runtime': 953.4756, 'train_samples_per_second': 17.299, 'train_steps_per_second': 1.079, 'total_flos': 0.0, 'train_loss': 0.6931475161115908, 'epoch': 2.9934545454545454})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85291202-1319-4ca6-b91c-36ad371e1939",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.save_model('Qwen2.5-3B-Instruct-DPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "612c6bb8-e155-4e9d-ada0-5a54780c937e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Donald Trump is an American businessman, investor, and former reality television host who served as the 45th President of the United States from 2017 to 2021'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"Give only one sentence answer.\\n\"\n",
    "input_text = prefix + \"Who is Donald Trump?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50, pad_token_id = tokenizer.eos_token_id)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7f89d47-c4d6-4f65-b2fd-3fcc27a52441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Benedetto Varchi was a Renaissance humanist scholar, writer, and physician.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt = \"Who is Donald Trump?\"\n",
    "input_text = prefix + \"What professions did Benedetto Varchi have?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=100, pad_token_id = tokenizer.eos_token_id)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip().split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1bc7b41-c15b-4b1e-86ee-d07ec3ca0b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Who is Donald Trump?\"\n",
    "input_text = prefix + \"What is the nationality of Benedetto Varchi?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=100, pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46f210dc-e1d2-4cb1-8d91-cc19d2b3450d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elsa Triolet met Vladimir Nabokov in 1928 that she would later marry.\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Who is Donald Trump?\"\n",
    "input_text = prefix + \"Who did Elsa Triolet meet in 1928 that she would later marry?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50, pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip().split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89543c2-6c71-47f2-8f47-3d51b2d75768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\tWhat discovery led Russell Alan Hulse to win the Nobel Prize in Physics?\n",
    "245\tIn what year did Russell Alan Hulse receive his PhD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74ea9506-54e0-443a-bba9-c5b7e08a930f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is American.\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Who is Donald Trump?\"\n",
    "input_text = prefix + \"What is the nationality of Benedetto Varchi?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50, pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip().split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25797746-c8da-40d5-8616-5b2294038833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Who is Donald Trump?\"\n",
    "input_text = prefix + \"In what country was Jorge Semprún Maura born?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50, pad_token_id = tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip().split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "134176ab-563b-42b1-97c0-be64285d75a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fda5bfd5134f17ba7694901ff20a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"microsoft-Phi-DPO\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\")\n",
    "# print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6932777-b78a-4533-9ab9-0409e9d5a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question):\n",
    "        input_text = prefix + question\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_length=256, pad_token_id = tokenizer.eos_token_id)\n",
    "        return(tokenizer.decode(outputs[0], skip_special_tokens=True).replace(input_text, \"\").strip().split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41c86c0d-8527-4dfc-a7e0-e4891c31a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "887ed62b-6800-4190-acc2-e9bab7c771d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_wikipedia_hard_retain_100.csv')\n",
    "prefix = \"Give only one sentence answer.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aad5b17b-3df8-4b67-87ca-bb2f6ede03b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for idx, row in test_data.iterrows():\n",
    "    print(idx)\n",
    "    question = row['question']\n",
    "    answer = generate_response(question)\n",
    "    responses.append([question, row['answer'], answer])\n",
    "output = pd.DataFrame(columns=['question', 'answer', 'generated'], data=responses)\n",
    "output.to_csv('Microsoft-Phi-Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7cff6cf-8cd3-4600-8312-131e9752bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "235eb6a5-6c92-459f-9127-ad0a2a0cf804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Average ROUGE-1 Score: 0.15876545599211347\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load evaluation CSV file\n",
    "eval_data = pd.read_csv('Microsoft-Phi-Results.csv')  # Replace with your CSV file name\n",
    "\n",
    "# Extract reference and generated answers\n",
    "test_references = eval_data['answer'].tolist()          # Ground truth (reference answers)\n",
    "test_predictions = eval_data['generated'].tolist()  # Model's predictions\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "\n",
    "# Calculate ROUGE-1 scores\n",
    "rouge1_scores = []\n",
    "for reference, prediction in zip(test_references, test_predictions):\n",
    "    score = scorer.score(reference, str(prediction))\n",
    "    rouge1_scores.append(score['rouge1'].fmeasure)\n",
    "\n",
    "# Compute overall average ROUGE-1 score\n",
    "average_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "print(f\"Overall Average ROUGE-1 Score: {average_rouge1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ec4d84b-e363-4e7e-9029-a86ff2fb0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4a5bc00-592e-43cd-b9a4-a21273978e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f62558fc-d479-45dd-b4f0-cdfbef3dbaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_data = pd.read_csv(\"llama3-3.2-3BInstruct-Results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd35904f-ddb7-4163-97ab-f19b63ab68ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall BLEU Score: 0.036155753457048675\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Step 1: Load your evaluation CSV\n",
    "# Replace 'evaluation.csv' with the path to your file\n",
    "# df = pd.read_csv('evaluation.csv')\n",
    "\n",
    "# Ensure your CSV has the necessary columns\n",
    "if 'answer' not in eval_data.columns or 'generated' not in eval_data.columns:\n",
    "    raise ValueError(\"CSV must contain 'answer' and 'generated_answer' columns.\")\n",
    "\n",
    "# Step 2: Prepare references and predictions\n",
    "# References should be a list of lists, each containing the tokenized ground truth\n",
    "references = [[ref.split()] for ref in eval_data['answer'].tolist()]\n",
    "# Predictions should be a list of tokenized generated answers\n",
    "predictions = [str(pred).split() for pred in eval_data['generated'].tolist()]\n",
    "\n",
    "# Step 3: Compute BLEU score\n",
    "bleu_score = corpus_bleu(references, predictions)\n",
    "\n",
    "print(f\"Overall BLEU Score: {bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9f9ba8b-7bd3-4646-9826-04a61dde9386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity: nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Step 1: Load your evaluation CSV\n",
    "# df = pd.read_csv('llama3-3.2-3BInstruct-Results.csv')\n",
    "\n",
    "# Ensure your CSV has the necessary columns\n",
    "if 'answer' not in eval_data.columns or 'generated' not in eval_data.columns:\n",
    "    raise ValueError(\"CSV must contain 'answer' and 'generated_answer' columns.\")\n",
    "\n",
    "# Step 2: Load the tokenizer and model\n",
    "# model_name = \"gpt2\"  # Replace with your model's name\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Step 3: Compute perplexity for each sentence\n",
    "perplexities = []\n",
    "\n",
    "for text in eval_data['generated']:\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "\n",
    "    \n",
    "    # Compute loss\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "        loss = outputs.loss.item()  # Cross-entropy loss\n",
    "    \n",
    "    # Convert loss to perplexity\n",
    "    perplexity = torch.exp(torch.tensor(loss)).item()\n",
    "    perplexities.append(perplexity)\n",
    "\n",
    "# Step 4: Compute average perplexity\n",
    "average_perplexity = sum(perplexities) / len(perplexities)\n",
    "print(f\"Average Perplexity: {average_perplexity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afedd91c-9ade-4495-ad5d-4d09e163a3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
